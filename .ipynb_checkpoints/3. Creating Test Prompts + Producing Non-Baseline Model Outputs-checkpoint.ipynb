{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2f734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, os, re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from utils import *\n",
    "import pickle\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# special characters that we do want to keep!\n",
    "spc = r\"\\w+|[^\\w\\s]|[\\n\\t\\r\\f\\v]\"\n",
    "\n",
    "# helper function for beautifying text, given a list of words/tokens\n",
    "def beautify(tokens):\n",
    "    \n",
    "    # 1. join our text together with spaces, but respecting new lines.\n",
    "    text = \" \".join(tokens).replace(\" \\n \", \"\\n\").replace(\" \\n\", \"\\n\").replace(\"\\n \", \"\\n\")\n",
    "    \n",
    "    # 2. regex modifications to delete unnecessary spaces with the join (e.g., after punctuation marks)\n",
    "    return re.sub(\n",
    "        r'(\\n)|(\\s+)([,:\\'?;!.])|([\\'\\s])\\s+', \n",
    "        lambda m: m.group(1) or m.group(3) or m.group(4), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c75f06",
   "metadata": {},
   "source": [
    "# Create our Test Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6241ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f4d2334bb64fb1b2c1c4a6e06879ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "torch.manual_seed(310); np.random.seed(310)\n",
    "\n",
    "# how many test prompts do we want?\n",
    "NUM_TEST_PROMPTS = 50\n",
    "\n",
    "# load in our input text\n",
    "with open(\"input.txt\", \"rt\") as file:\n",
    "    corpus = file.readlines()\n",
    "\n",
    "# sample 100 test prompts (each one line)\n",
    "test_corpus = corpus[int(0.9*len(corpus)):]\n",
    "\n",
    "# concatenate the chunks that end with \"\\n\"\n",
    "prompts = []\n",
    "current_prompt = \"\"\n",
    "for line in tqdm(test_corpus):\n",
    "    \n",
    "    if line == \"\\n\":\n",
    "        prompts.append(current_prompt)\n",
    "        current_prompt = \"\"\n",
    "    else:\n",
    "        current_prompt += line\n",
    "        \n",
    "# pick a random 50 of them\n",
    "test_prompts = np.random.choice(a=prompts, size=NUM_TEST_PROMPTS)\n",
    "with open(\"test_prompts.pickle\", \"wb\") as file:\n",
    "    pickle.dump(test_prompts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb2ac1b",
   "metadata": {},
   "source": [
    "# Generate Outputs for Every Model (Non-Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d712c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output for mc=5, embed_size=960, freeze_type=None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56b7cad33af45108545feda7d54dba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "torch.manual_seed(310); np.random.seed(310)\n",
    "\n",
    "# dictionary to store all of our outputs\n",
    "outputs = {}\n",
    "\n",
    "# load in the biggest word2vec model because that is what we will use for finding \"similar\" tokens\n",
    "super_w2v = Word2Vec.load(\"word2vec_models/mc=1_vs=1152.model\")\n",
    "\n",
    "# go thru each model\n",
    "for mc in [1, 3, 5]:\n",
    "    for embed_size in [192, 384, 576, 768, 960]:\n",
    "        for freeze_type in [True, False, None]:\n",
    "            \n",
    "            # status update\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Generating output for mc={mc}, embed_size={embed_size}, freeze_type={str(freeze_type)}.\")\n",
    "            \n",
    "            # create another dictionary in \"outputs\"\n",
    "            outputs[(mc, embed_size, freeze_type)] = {}\n",
    "            \n",
    "            # 1. load in the PyTorch model + set to evaluation mode\n",
    "            fname = f\"mc={mc}_embed-size={embed_size}_freeze-type={freeze_type}\"\n",
    "            model = torch.load(f\"models/{fname}/model.pth\", weights_only=False)\n",
    "            model.eval()\n",
    "            \n",
    "            # 2. load in the corresponding word2vec model so that we can tokenize properly\n",
    "            w2v = Word2Vec.load(f\"word2vec_models/mc={mc}_vs={embed_size}.model\")\n",
    "            \n",
    "            # go thru each of our test prompts\n",
    "            for test_prompt in tqdm(test_prompts):\n",
    "                \n",
    "                # a. split up our text into word + punctuation tokens\n",
    "                splitted = re.findall(spc, test_prompt)\n",
    "                \n",
    "                # b. tokenize into the TRUNCATED tokenizer!\n",
    "                token_ids = []\n",
    "                for token in splitted:\n",
    "                    try:\n",
    "                        \n",
    "                        # directly encode the token_id if it is in our vocabulary\n",
    "                        token_ids.append(w2v.wv.key_to_index[token])\n",
    "                    except:\n",
    "                        \n",
    "                        # if not in vocabulary, find the closest word that is in our vocabulary\n",
    "                        closest_token_id = np.argmax(\n",
    "                            [super_w2v.wv.similarity(token, reference_word) \n",
    "                             for reference_word in w2v.wv.index_to_key])\n",
    "                        token_ids.append(closest_token_id)\n",
    "                token_ids = torch.tensor(token_ids, dtype=torch.long, device=\"cuda\").reshape(1, -1)\n",
    "                \n",
    "                # c. generate our text + extract out the words from the ids\n",
    "                output = model.generate(\n",
    "                    token_ids=token_ids, \n",
    "                    max_new_tokens=3*len(token_ids.flatten())).cpu().flatten()\n",
    "                tokens = [w2v.wv.index_to_key[idx] for idx in output]\n",
    "                \n",
    "                # d. join our tokens back together using sentences + beautify\n",
    "                outputs[(mc, embed_size, freeze_type)][test_prompt] = beautify(tokens)\n",
    "                \n",
    "            # after each model, clear our cache\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "# save our outputs\n",
    "with open(\"outputs.pickle\", \"wb\") as file:\n",
    "    pickle.dump(outputs, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Afterburner)",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
